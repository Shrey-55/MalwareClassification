# metrics.py
# This file contains custom metrics and evaluation functions.

from sklearn.metrics import accuracy_score, f1_score, confusion_matrix

def compute_accuracy(y_true, y_pred):
    """
    Compute the accuracy of the predictions.

    Args:
    - y_true (numpy.ndarray or list): True labels.
    - y_pred (numpy.ndarray or list): Predicted labels.

    Returns:
    - float: Accuracy score.
    """
    return accuracy_score(y_true, y_pred)

def compute_f1_score(y_true, y_pred, average='macro'):
    """
    Compute the F1 score of the predictions.

    Args:
    - y_true (numpy.ndarray or list): True labels.
    - y_pred (numpy.ndarray or list): Predicted labels.
    - average (str): Type of averaging performed ('micro', 'macro', 'weighted').

    Returns:
    - float: F1 score.
    """
    return f1_score(y_true, y_pred, average=average)

def get_confusion_matrix(y_true, y_pred):
    """
    Compute the confusion matrix for the given predictions.

    Args:
    - y_true (numpy.ndarray or list): True labels.
    - y_pred (numpy.ndarray or list): Predicted labels.

    Returns:
    - numpy.ndarray: Confusion matrix.
    """
    return confusion_matrix(y_true, y_pred)
